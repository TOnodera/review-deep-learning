{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173648fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
    "\n",
    "class TwoLayerNet:\n",
    "  def __init__(self, input_size, hidden_size, output_size):\n",
    "    I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "    # 初期化\n",
    "    W1 = 0.01 * np.random.randn(I, H)\n",
    "    b1 = np.zeros(H)\n",
    "    W2 = 0.01 * np.random.randn(H, O)\n",
    "    b2 = np.zeros(0)\n",
    "    \n",
    "    # レイヤの生成\n",
    "    self.layers = [\n",
    "      Affine(W1, b1),\n",
    "      Sigmoid(),\n",
    "      Affine(W2, b2),\n",
    "    ]\n",
    "\n",
    "    # 損失関数の生成\n",
    "    self.loss_layer = SoftmaxWithLoss()\n",
    "    \n",
    "    # 全ての重みと勾配をリストにまとめる\n",
    "    self.params, self.grads = [], []\n",
    "    for layer in self.layers:\n",
    "      self.params += layer.params\n",
    "      self.grads += layer.grads\n",
    "\n",
    "  def predict(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "  \n",
    "  def forward(self, x, t):\n",
    "    score = self.predict(x)\n",
    "    loss = self.loss_layer.forward(score, t)\n",
    "    return loss\n",
    "\n",
    "  def backword(self, dout=1):\n",
    "    dout = self.loss_layer.backward(dout)\n",
    "    for layer in reversed(self.layers):\n",
    "      dout = layer.backward(dout)\n",
    "    return dout\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
