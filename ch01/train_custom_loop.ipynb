{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/TOnodera/review-deep-learning.git\n",
        "!ls /content/review-deep-learning/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VszEnXaoNj1N",
        "outputId": "c0ca37c4-b200-40b2-c775-afe40cc1d7c5"
      },
      "id": "VszEnXaoNj1N",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ch01  common  dataset  Dockerfile  env.test.ipynb  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "45b05552",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45b05552",
        "outputId": "dec8601a-9649-4456-d8de-ec71fbe88754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.1256062166823237\n",
            "Epoch 2, Loss: 1.1255632260656587\n",
            "Epoch 3, Loss: 1.1224626091142815\n",
            "Epoch 4, Loss: 1.120918633796049\n",
            "Epoch 5, Loss: 1.1191949092958782\n",
            "Epoch 6, Loss: 1.1224064214816991\n",
            "Epoch 7, Loss: 1.127647816742461\n",
            "Epoch 8, Loss: 1.1252707914374427\n",
            "Epoch 9, Loss: 1.1243885443546662\n",
            "Epoch 10, Loss: 1.124829267041892\n",
            "Epoch 11, Loss: 1.124103316304937\n",
            "Epoch 12, Loss: 1.1227896048713435\n",
            "Epoch 13, Loss: 1.120083859885082\n",
            "Epoch 14, Loss: 1.1169836832205715\n",
            "Epoch 15, Loss: 1.112133949245536\n",
            "Epoch 16, Loss: 1.1072867188323008\n",
            "Epoch 17, Loss: 1.0984635708988946\n",
            "Epoch 18, Loss: 1.0884592236871753\n",
            "Epoch 19, Loss: 1.0798113229468613\n",
            "Epoch 20, Loss: 1.0692464521820642\n",
            "Epoch 21, Loss: 1.0587766546869257\n",
            "Epoch 22, Loss: 1.0477942121638297\n",
            "Epoch 23, Loss: 1.0366919190756536\n",
            "Epoch 24, Loss: 1.0261074490849436\n",
            "Epoch 25, Loss: 1.0180048792776883\n",
            "Epoch 26, Loss: 1.008676128543063\n",
            "Epoch 27, Loss: 0.9993098187333117\n",
            "Epoch 28, Loss: 0.9909229450177859\n",
            "Epoch 29, Loss: 0.9837700816875814\n",
            "Epoch 30, Loss: 0.9760040643336498\n",
            "Epoch 31, Loss: 0.9695944720524522\n",
            "Epoch 32, Loss: 0.9632035184333718\n",
            "Epoch 33, Loss: 0.957433403268557\n",
            "Epoch 34, Loss: 0.9522718414029592\n",
            "Epoch 35, Loss: 0.9464349401368494\n",
            "Epoch 36, Loss: 0.9408392991174037\n",
            "Epoch 37, Loss: 0.9358445815367625\n",
            "Epoch 38, Loss: 0.9312733180153621\n",
            "Epoch 39, Loss: 0.9261352724534415\n",
            "Epoch 40, Loss: 0.9218075628885477\n",
            "Epoch 41, Loss: 0.9178571743840037\n",
            "Epoch 42, Loss: 0.9140854411817936\n",
            "Epoch 43, Loss: 0.9105235602527562\n",
            "Epoch 44, Loss: 0.9066144679813803\n",
            "Epoch 45, Loss: 0.9030970066467174\n",
            "Epoch 46, Loss: 0.8993821222034574\n",
            "Epoch 47, Loss: 0.8956228159107978\n",
            "Epoch 48, Loss: 0.8922337023164225\n",
            "Epoch 49, Loss: 0.888777077786949\n",
            "Epoch 50, Loss: 0.8854517558697999\n",
            "Epoch 51, Loss: 0.8821122219726234\n",
            "Epoch 52, Loss: 0.8789859529460096\n",
            "Epoch 53, Loss: 0.8763167618253153\n",
            "Epoch 54, Loss: 0.8737212307234036\n",
            "Epoch 55, Loss: 0.8709711152718836\n",
            "Epoch 56, Loss: 0.8682446518544588\n",
            "Epoch 57, Loss: 0.8654242900170023\n",
            "Epoch 58, Loss: 0.8625791059252347\n",
            "Epoch 59, Loss: 0.8601164460712226\n",
            "Epoch 60, Loss: 0.8574714507519595\n",
            "Epoch 61, Loss: 0.8551191286941383\n",
            "Epoch 62, Loss: 0.8528726765764777\n",
            "Epoch 63, Loss: 0.8504844528973546\n",
            "Epoch 64, Loss: 0.8483638452440012\n",
            "Epoch 65, Loss: 0.8464788414419513\n",
            "Epoch 66, Loss: 0.8442472720697339\n",
            "Epoch 67, Loss: 0.842316772206707\n",
            "Epoch 68, Loss: 0.8401118098642675\n",
            "Epoch 69, Loss: 0.8380097771070018\n",
            "Epoch 70, Loss: 0.8361121277213867\n",
            "Epoch 71, Loss: 0.8339399740933096\n",
            "Epoch 72, Loss: 0.8319839761700618\n",
            "Epoch 73, Loss: 0.8297360361254539\n",
            "Epoch 74, Loss: 0.82770675187051\n",
            "Epoch 75, Loss: 0.8255998868881151\n",
            "Epoch 76, Loss: 0.8234223001078617\n",
            "Epoch 77, Loss: 0.8217531066191612\n",
            "Epoch 78, Loss: 0.8194853924444901\n",
            "Epoch 79, Loss: 0.8177171853982262\n",
            "Epoch 80, Loss: 0.8154831314771207\n",
            "Epoch 81, Loss: 0.8132575377483384\n",
            "Epoch 82, Loss: 0.8114399856770196\n",
            "Epoch 83, Loss: 0.8091271714162228\n",
            "Epoch 84, Loss: 0.8069109843443238\n",
            "Epoch 85, Loss: 0.8046239230067641\n",
            "Epoch 86, Loss: 0.8022119135921746\n",
            "Epoch 87, Loss: 0.7998740737418398\n",
            "Epoch 88, Loss: 0.7976981016627687\n",
            "Epoch 89, Loss: 0.795343215231061\n",
            "Epoch 90, Loss: 0.7929207924456283\n",
            "Epoch 91, Loss: 0.7903324641689066\n",
            "Epoch 92, Loss: 0.7878338397912262\n",
            "Epoch 93, Loss: 0.7852216377854575\n",
            "Epoch 94, Loss: 0.7824916372245175\n",
            "Epoch 95, Loss: 0.7798331085415946\n",
            "Epoch 96, Loss: 0.7770880406011996\n",
            "Epoch 97, Loss: 0.774359616037434\n",
            "Epoch 98, Loss: 0.7715768039984031\n",
            "Epoch 99, Loss: 0.7686767724661957\n",
            "Epoch 100, Loss: 0.7657900639584967\n",
            "Epoch 101, Loss: 0.7627722760783058\n",
            "Epoch 102, Loss: 0.7597453123521137\n",
            "Epoch 103, Loss: 0.756772750268686\n",
            "Epoch 104, Loss: 0.7537556367711689\n",
            "Epoch 105, Loss: 0.7507308727747077\n",
            "Epoch 106, Loss: 0.7475210860515396\n",
            "Epoch 107, Loss: 0.7443027972942543\n",
            "Epoch 108, Loss: 0.741185283136183\n",
            "Epoch 109, Loss: 0.7380883896246406\n",
            "Epoch 110, Loss: 0.7350104469483378\n",
            "Epoch 111, Loss: 0.7318050982441026\n",
            "Epoch 112, Loss: 0.7286197593292504\n",
            "Epoch 113, Loss: 0.7253779471665182\n",
            "Epoch 114, Loss: 0.7222392901210295\n",
            "Epoch 115, Loss: 0.7189690798178994\n",
            "Epoch 116, Loss: 0.715744329132315\n",
            "Epoch 117, Loss: 0.7124925770463125\n",
            "Epoch 118, Loss: 0.7093195398522787\n",
            "Epoch 119, Loss: 0.706118063167736\n",
            "Epoch 120, Loss: 0.7030270508255484\n",
            "Epoch 121, Loss: 0.699827153215898\n",
            "Epoch 122, Loss: 0.6966925600222903\n",
            "Epoch 123, Loss: 0.6935127907944065\n",
            "Epoch 124, Loss: 0.6904446830638368\n",
            "Epoch 125, Loss: 0.6873225833020368\n",
            "Epoch 126, Loss: 0.684238609565889\n",
            "Epoch 127, Loss: 0.6810855728533278\n",
            "Epoch 128, Loss: 0.6779613424064409\n",
            "Epoch 129, Loss: 0.6748905886867476\n",
            "Epoch 130, Loss: 0.6718353887690581\n",
            "Epoch 131, Loss: 0.6687861740226252\n",
            "Epoch 132, Loss: 0.6657429236373076\n",
            "Epoch 133, Loss: 0.6627745204792983\n",
            "Epoch 134, Loss: 0.6598476393851693\n",
            "Epoch 135, Loss: 0.6569621669432589\n",
            "Epoch 136, Loss: 0.6540754789589136\n",
            "Epoch 137, Loss: 0.6512266291520782\n",
            "Epoch 138, Loss: 0.6483630318350179\n",
            "Epoch 139, Loss: 0.6455126081886108\n",
            "Epoch 140, Loss: 0.6426368833397247\n",
            "Epoch 141, Loss: 0.6397774390637496\n",
            "Epoch 142, Loss: 0.6370002925032202\n",
            "Epoch 143, Loss: 0.6342008175330571\n",
            "Epoch 144, Loss: 0.6314655292906003\n",
            "Epoch 145, Loss: 0.6287217179720784\n",
            "Epoch 146, Loss: 0.6260353338202089\n",
            "Epoch 147, Loss: 0.623342944315091\n",
            "Epoch 148, Loss: 0.6206806860360586\n",
            "Epoch 149, Loss: 0.6180185131865943\n",
            "Epoch 150, Loss: 0.6153913682010556\n",
            "Epoch 151, Loss: 0.6127934623066587\n",
            "Epoch 152, Loss: 0.6102266696571568\n",
            "Epoch 153, Loss: 0.6076564092980627\n",
            "Epoch 154, Loss: 0.6051233125681326\n",
            "Epoch 155, Loss: 0.6026068433685109\n",
            "Epoch 156, Loss: 0.6000796043290658\n",
            "Epoch 157, Loss: 0.5975895141602973\n",
            "Epoch 158, Loss: 0.595103533439853\n",
            "Epoch 159, Loss: 0.5926727375950858\n",
            "Epoch 160, Loss: 0.5902070053282167\n",
            "Epoch 161, Loss: 0.5878104134870001\n",
            "Epoch 162, Loss: 0.5853994862893167\n",
            "Epoch 163, Loss: 0.5831086243124226\n",
            "Epoch 164, Loss: 0.5807719897304212\n",
            "Epoch 165, Loss: 0.5784443290916786\n",
            "Epoch 166, Loss: 0.5761340459305488\n",
            "Epoch 167, Loss: 0.573813559164648\n",
            "Epoch 168, Loss: 0.571511906009697\n",
            "Epoch 169, Loss: 0.5692303309272365\n",
            "Epoch 170, Loss: 0.5669899982096923\n",
            "Epoch 171, Loss: 0.564785351829429\n",
            "Epoch 172, Loss: 0.5625641039710851\n",
            "Epoch 173, Loss: 0.5603782634975483\n",
            "Epoch 174, Loss: 0.5582111807996462\n",
            "Epoch 175, Loss: 0.5560513143654149\n",
            "Epoch 176, Loss: 0.553886878764047\n",
            "Epoch 177, Loss: 0.5517613831901057\n",
            "Epoch 178, Loss: 0.5496542253427532\n",
            "Epoch 179, Loss: 0.5475496680589405\n",
            "Epoch 180, Loss: 0.5454704227547161\n",
            "Epoch 181, Loss: 0.5434321216753253\n",
            "Epoch 182, Loss: 0.5413958249967856\n",
            "Epoch 183, Loss: 0.5394005960673154\n",
            "Epoch 184, Loss: 0.5374166052681307\n",
            "Epoch 185, Loss: 0.5354278166497078\n",
            "Epoch 186, Loss: 0.533503761255662\n",
            "Epoch 187, Loss: 0.5315751687194368\n",
            "Epoch 188, Loss: 0.5296332005027136\n",
            "Epoch 189, Loss: 0.5277089059568251\n",
            "Epoch 190, Loss: 0.5258226032913293\n",
            "Epoch 191, Loss: 0.5239230664218251\n",
            "Epoch 192, Loss: 0.5220775760015071\n",
            "Epoch 193, Loss: 0.5202114063036333\n",
            "Epoch 194, Loss: 0.518351529874699\n",
            "Epoch 195, Loss: 0.5165284980731858\n",
            "Epoch 196, Loss: 0.5147158117238356\n",
            "Epoch 197, Loss: 0.5129337474297696\n",
            "Epoch 198, Loss: 0.5111147278811302\n",
            "Epoch 199, Loss: 0.5093488412093511\n",
            "Epoch 200, Loss: 0.5075894861458802\n",
            "Epoch 201, Loss: 0.5058337023452031\n",
            "Epoch 202, Loss: 0.5041064944452377\n",
            "Epoch 203, Loss: 0.5023891054473166\n",
            "Epoch 204, Loss: 0.5006810624823199\n",
            "Epoch 205, Loss: 0.4990074166726882\n",
            "Epoch 206, Loss: 0.49733012281832645\n",
            "Epoch 207, Loss: 0.4956737595729359\n",
            "Epoch 208, Loss: 0.49400071155511605\n",
            "Epoch 209, Loss: 0.4923615449588597\n",
            "Epoch 210, Loss: 0.49072818742431396\n",
            "Epoch 211, Loss: 0.4891214433792102\n",
            "Epoch 212, Loss: 0.4875103006173661\n",
            "Epoch 213, Loss: 0.48591565816104304\n",
            "Epoch 214, Loss: 0.4843233410961223\n",
            "Epoch 215, Loss: 0.4827533906437498\n",
            "Epoch 216, Loss: 0.4811845491741068\n",
            "Epoch 217, Loss: 0.4796242410516945\n",
            "Epoch 218, Loss: 0.47809179888157255\n",
            "Epoch 219, Loss: 0.4765444583495366\n",
            "Epoch 220, Loss: 0.47500248100900205\n",
            "Epoch 221, Loss: 0.47350521181173744\n",
            "Epoch 222, Loss: 0.4720052233904308\n",
            "Epoch 223, Loss: 0.4705264997092994\n",
            "Epoch 224, Loss: 0.4690623028653694\n",
            "Epoch 225, Loss: 0.46758501273627795\n",
            "Epoch 226, Loss: 0.4661204335604751\n",
            "Epoch 227, Loss: 0.46469486624104867\n",
            "Epoch 228, Loss: 0.46325360179188774\n",
            "Epoch 229, Loss: 0.4617944143133782\n",
            "Epoch 230, Loss: 0.46039022586508915\n",
            "Epoch 231, Loss: 0.4589661269251487\n",
            "Epoch 232, Loss: 0.4575738139533688\n",
            "Epoch 233, Loss: 0.45618634150619697\n",
            "Epoch 234, Loss: 0.4547985396104378\n",
            "Epoch 235, Loss: 0.4534227274920051\n",
            "Epoch 236, Loss: 0.45204638855087026\n",
            "Epoch 237, Loss: 0.4507102942894808\n",
            "Epoch 238, Loss: 0.4493721460687096\n",
            "Epoch 239, Loss: 0.448041493943884\n",
            "Epoch 240, Loss: 0.4467488875546765\n",
            "Epoch 241, Loss: 0.4454498169370887\n",
            "Epoch 242, Loss: 0.44414054279977294\n",
            "Epoch 243, Loss: 0.44283852473096164\n",
            "Epoch 244, Loss: 0.44155649157823823\n",
            "Epoch 245, Loss: 0.4402907905881928\n",
            "Epoch 246, Loss: 0.43902117785791045\n",
            "Epoch 247, Loss: 0.4377556443084172\n",
            "Epoch 248, Loss: 0.43652280065823856\n",
            "Epoch 249, Loss: 0.43528291028827737\n",
            "Epoch 250, Loss: 0.43406339180104414\n",
            "Epoch 251, Loss: 0.4328658665865157\n",
            "Epoch 252, Loss: 0.43164234977661387\n",
            "Epoch 253, Loss: 0.4304158059312957\n",
            "Epoch 254, Loss: 0.42920495663440333\n",
            "Epoch 255, Loss: 0.42800285929166343\n",
            "Epoch 256, Loss: 0.42681192972179416\n",
            "Epoch 257, Loss: 0.42562308328341836\n",
            "Epoch 258, Loss: 0.42445661120032735\n",
            "Epoch 259, Loss: 0.42332726945064336\n",
            "Epoch 260, Loss: 0.42216165138493\n",
            "Epoch 261, Loss: 0.4210231131249356\n",
            "Epoch 262, Loss: 0.41987653878185804\n",
            "Epoch 263, Loss: 0.41872912983922234\n",
            "Epoch 264, Loss: 0.41762339647888563\n",
            "Epoch 265, Loss: 0.41649302143166733\n",
            "Epoch 266, Loss: 0.41537891251652936\n",
            "Epoch 267, Loss: 0.4142723625253366\n",
            "Epoch 268, Loss: 0.41316435062728313\n",
            "Epoch 269, Loss: 0.412050746260561\n",
            "Epoch 270, Loss: 0.4109649379454407\n",
            "Epoch 271, Loss: 0.4098777430098678\n",
            "Epoch 272, Loss: 0.408805596571727\n",
            "Epoch 273, Loss: 0.407733916861564\n",
            "Epoch 274, Loss: 0.4066955636655881\n",
            "Epoch 275, Loss: 0.4056285374072142\n",
            "Epoch 276, Loss: 0.4045784385505027\n",
            "Epoch 277, Loss: 0.4035661888462572\n",
            "Epoch 278, Loss: 0.4025206088324739\n",
            "Epoch 279, Loss: 0.4014749278454929\n",
            "Epoch 280, Loss: 0.40044934660051135\n",
            "Epoch 281, Loss: 0.3994309282309179\n",
            "Epoch 282, Loss: 0.39843051823430925\n",
            "Epoch 283, Loss: 0.3974174769948099\n",
            "Epoch 284, Loss: 0.3964189097282182\n",
            "Epoch 285, Loss: 0.3954263721493213\n",
            "Epoch 286, Loss: 0.3944312720277265\n",
            "Epoch 287, Loss: 0.3934484329402247\n",
            "Epoch 288, Loss: 0.3924885057164549\n",
            "Epoch 289, Loss: 0.39152469709078763\n",
            "Epoch 290, Loss: 0.39057022240666944\n",
            "Epoch 291, Loss: 0.3896127707640274\n",
            "Epoch 292, Loss: 0.3886638473855672\n",
            "Epoch 293, Loss: 0.3877134250952428\n",
            "Epoch 294, Loss: 0.3867738538203619\n",
            "Epoch 295, Loss: 0.38585524131026394\n",
            "Epoch 296, Loss: 0.3849240507212339\n",
            "Epoch 297, Loss: 0.38403093438260266\n",
            "Epoch 298, Loss: 0.38311674562332193\n",
            "Epoch 299, Loss: 0.3821922320825329\n",
            "Epoch 300, Loss: 0.3812800898021153\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/review-deep-learning/ch01\")\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "\n",
        "# ハイパーパラメータの設定\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "\n",
        "# データの読み込み、モデルとオプティマイザーの生成\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "# 学習で使用する変数\n",
        "data_size = len(x)\n",
        "max_iters = data_size // batch_size\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    idx = np.random.permutation(data_size)\n",
        "    x = x[idx]\n",
        "    t = t[idx]\n",
        "\n",
        "    for iters in range(max_iters):\n",
        "        batch_x = x[iters * batch_size : (iters + 1) * batch_size]\n",
        "        batch_t = t[iters * batch_size : (iters + 1) * batch_size]\n",
        "\n",
        "        # 勾配を求めてパラメーターを更新\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "\n",
        "        # 損失の記録\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # エポックごとの損失を記録\n",
        "    avg_loss = total_loss / loss_count\n",
        "    loss_list.append(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}